title: 大数据
author: 乡村程序员
tags:
  - 分布式系统
  - ''
categories:
  - 分布式系统
date: 2021-06-03 20:06:00
---

# 大数据

大数据的特点：

- Volume，传统的数据仓库计数处理GB到TB级别的数据，大数据技术处理的数据量往往超过PB。数据容量增长速度大大超过了硬件计数的发展速度，以至于引发了数据存储和处理的危机。
- Variety：数据类型多。原来的数据都可以用二维表结构存储在数据库中,如常用的Excel软件所处理的数据，称为结构化数据。但是现在更多互联网多媒体应用的出现，使诸如图片、声音和视频等非结构化数据占到了很大比重。
- Velocity，数据增长迅速。如果说大数据的特点是海量和非结构化，那也是不全面的。大数据带来的挑战还在于它的实时处理。
- Value，价值密度低。以连续不间断的监控视频为例，可能有用的数据仅仅有一两秒钟。

## MapReduce

Mapreduce使得普通程序员可以在不了解分布式底层细节的前提下开发分布式程序。使用者只需要编写两个称为MapReduce的函数即可，MapReduce框架会自动处理数据划分、多机并行执行、任务之间的协调，并且能够处理某个任务执行失败或者机器出现故障的情况。

MapReduce框架 包含三种角色：

主控进程（Master）：执行任务划分、调度、任务之间的协调等等；

Map工作进程（Map）、Reduce工作进程（Reduce）。

工作流程如下：

1）首先从用户提交的程序fork出主控进程，主控进程启动后将切分任务并根据输入文件所在的位置和集群信息选择机器fork出Map或者Reduce进程。用户提交的程序可以根据不同的命令行参数执行不同的行为。

2）主控进程将切分好的任务分配给Map进程和Reduce进程执行，任务切分和任务分配可以并行执行。

3）Map进程执行Map任务：读取相应的输入文件，根据指定的输入格式不断地读取<Key,Value>对并对每个<key,value>对执行用户自定义的Map函数。

4）Map进程执行用户定义的Map函数：不断地往本地内存缓冲区输出中间<key,value>对结果，等到缓冲区超过一定大小时写入到本地缓存中。Map进程根据分割函数将中间结果组织成R份，便于后续Reduce进程获取。

5）Map任务执行完成时，Map进程通过心跳向主控进程汇报，主控进程进一步将该消息通知Reduce进程。Reduce进程向Map进程通过心跳向主控进程汇报，主控进程进一步将该消息通知Reduce进程。Reduce进程向Map进程请求传输生成的中间结果数据。这个过程称为Shuffle。当Reduce进程获取完所有的Map任务生成的中间结果时，需要进行排序操作。

6）Reduce进程执行Reduce任务：对中间结果的每一个相同的key及value集合，执行用户自定义的Reduce函数。Reduce函数的输出结果被写入到最终的输出结果，例如分布式文件系统或者分布式表格。

该框架实现时主要做了两点优化：

- 本地化：尽量将任务分配给离输入文件最近的Map进程，如同一台机器或者同一个机架。通过本地化策略，能够大大减少传输的数据量。
- 备份任务：如果某个Map或者Reduce任务执行的时间较长，主控进程会生成一个该任务的备份并分配给另外一个空闲的Map或者Reduce进程。在大集群环境下，即使所有机器的配置相同，机器的负载不同也会导致处理能力相差很大，通过备份任务减少“拖后腿”的任务，从而降低整个作业的总体执行时间。



## 流式计算

MapReduce及其扩展解决了离线批处理问题，但是无法保证实时性。对于实时性要求高的场景，可以采用流式计算或者实时分析系统进行处理。

流式计算解决在线聚合、在线过滤等问题，流氏计算同时具有存储系统和计算系统的特点，经常应用在一些类似反作弊、交易异常监控等场景。

### 原理

MapReduce系统主要解决的是对静态数据的批量处理，当MapReduce作业启动时，已经准备好了输入数据，比如保存在分布式文件系统上。而流式计算系统在启动时，输入数据一般并没有完全到位，而是经由外部数据流源源不断地流入。另外，流式计算并不像批处理系统那样，重视数据处理的总吞吐量，而是更加重视对数据处理的延迟。

源数据写入到流处理节点，流处理节点内部运行用户自定义的钩子函数对输入流进行处理，处理完后根据一定的规则转发给下游的流处理节点继续处理。另外，系统中往往还有管理节点，用来管理流处理节点的状态以及节点之间的路由规则。

典型钩子函数包括：

- 聚合函数：计算最近一段时间窗口内的数据的聚合值。
- 过滤函数：过滤最近一段时间窗口内满足某些特性的数据。

如果考虑机器故障，问题变得复杂。上游的处理节点出席那故障时，下游有两种选择：第一种选择是等待上游恢复服务，保证数据一致性；第二种选择是继续处理，优先保证可用性，等到上游恢复后再修复错误的计算结果。

流处理节点可以通过主备同步的方式容错，即将数据强同步到备机，如果主机出现故障，备机自动切换为主机继续提供服务。然而，这种方式的代价很高，且流式处理系统往往对错误有一定的容忍度，实际应用时经常选择其他代价更低的容错方式。

## 实时分析

海量数据离线分析对于MapReduce这样的批处理系统挑战并不大，如果要求实时，又分为两种情况：如果查询模式单一，那么，可以通过MapReduce预处理后将最终结果导入到在线系统提供实时查询；如果查询模式复杂，例如涉及多个列任意组合查询，那么，只能通过实时分析系统解决。实时分析系统融合了并行数据库和云计算这类技术，能够从海量数据中快速分析出汇总结果。

### MPP架构

并行数据库往往采用MPP（Massively Parallel Processing，大规模并行处理）架构。MPP架构是一种不共享的结构，每个节点可以运行自己的操作系统、数据库等。每个节点内的CPU不能访问一个节点的内存，节点之间的信息交互是通过节点互联网络实现的。

将数据分布到多个节点，每个节点扫描本地数据，并由Merge操作符执行结果汇总。

常见的数据分布算法有两种：

- 范围分区（Range partitioning）：按照范围划分数据。
- 哈希分区（Hashing）：根据哈希函数计算结果将每个元组分配给相应的节点。

Merge操作符：系统中存在一个或者多个合并节点，它会发送命令给各个数据分片请求相应的数据。每个数据片所在节点扫描本地数据，排序后回复合并节点，由合并节点通过merge操作符执行数据汇总。

如果Merge节点处理的数据特别大、可以通过Split操作符将数据划分到多个节点，每个节点对一部分数据执行group by、join等操作后再合并最终结果。

并行数据库的SQL查询和MapReduce计算有些类似，可以认为MapReduce模型是一种更高层次的抽象。由于考虑问题的角度不同，并行数据库处理的SQL查询执行时间通常很短，出现异常时整个操作重做即可。











